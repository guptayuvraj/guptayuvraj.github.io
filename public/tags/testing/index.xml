<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Testing on Blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://guptayuvraj.github.io/tags/testing/</link>
    <language>en-us</language>
    <author>Yuvraj Gupta</author>
    
    <updated>Fri, 05 Aug 2016 00:00:00 UTC</updated>
    
    <item>
      <title>Big Data Testing - Use Case</title>
      <link>http://guptayuvraj.github.io/post/Big-Data-UseCase/</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/Big-Data-UseCase/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Use Case 1: You are deploying a Hadoop cluster of 200 nodes in production.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This blog is the second blog in the series of Big Data Testing. Kindly visit the first blog (&lt;a href=&#34;http://guptayuvraj.github.io/post/Big-Data-Testing/&#34;&gt;http://guptayuvraj.github.io/post/Big-Data-Testing/&lt;/a&gt;) to know about Big Data Testing along with various scenarios to test in Big Data ecosystem.&lt;/p&gt;

&lt;p&gt;In this blog we will look at a common use-case used for testing in production environment. It is a fairly easy practical scenario which you can face. We will cover the following topics in this blog:-&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#PS&#34;&gt;Problem Statement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#TT&#34;&gt;Traditional Method of Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#CTT&#34;&gt;Challenges in Traditional Method of Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#UA&#34;&gt;Use of Automation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#SD&#34;&gt;What will Script do?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#AS&#34;&gt;Creating Automation Script as Sample&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#SCF&#34;&gt;Structure of Configuration File&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#PSC&#34;&gt;Python Script&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#EPS&#34;&gt;Explanation of Python Script&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#AAS&#34;&gt;Advantages of Automation Script&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&#34;PS&#34;&gt; &lt;strong&gt;Problem Statement&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You need to validate that all the configuration files (5 configuration files in every server) across multiple servers in cluster is correct and consistent. Verify the configuration files are in desired location and content of each configuration files are same across every server.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;TT&#34;&gt; &lt;strong&gt;Traditional Method of Testing?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Login into every server and check whether all the configuration files are present is desired directory location and manually check the content of all configuration files present in every server across cluster.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;CTT&#34;&gt; &lt;strong&gt;Challenges in Traditional Method of Testing&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Time Consuming&lt;/li&gt;
&lt;li&gt;Tedious work&lt;/li&gt;
&lt;li&gt;Impossible to verify contents of each configuration file across each server&lt;/li&gt;
&lt;li&gt;Impossible to find difference in configuration files across servers&lt;/li&gt;
&lt;li&gt;Tough to find the problem post production and fixing it will require downtime and loss in data affecting business significantly.&lt;/li&gt;
&lt;li&gt;Tough to find the cause of problem and tougher to know whether problem is due to configuration files or not and if via configuration files then which configuration file on which server.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&#34;UA&#34;&gt;&lt;strong&gt;Use of Automation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create a script and a configuration file for the script. Configuration file will contain list of servers, list of files to check and directory in which files will be present.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;SD&#34;&gt;&lt;strong&gt;What will Script Do?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Initially, It will fetch the first server host address from configuration file and login into that server, fetch the file from the server as per the location of the file mentioned in configuration file, copy file from that server to local machine. Again it will fetch the second server host address from configuration file and login into that server, check whether configuration file is present in desired location as mentioned in configuration file, copy configuration files from second server to local machine and compare the configuration files received from both servers. The process continues for all the servers as mentioned in configuration file where it will fetch files and compare the files to validate whether there is any mismatch between files present across each server. Also this process continues for checking multiple files across each server.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;AS&#34;&gt;&lt;strong&gt;Creating Automation Script as Sample&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We will be creating scripts using Python in this use-case. You will require to create 2 files i.e. 1 configuration file and 1 script file.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;SCF&#34;&gt;&lt;strong&gt;Structure of Configuration file:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[server]
server1=nameofserver1.com
server2=nameofserver2.com
server3=nameofserver3.com 
server4=nameofserver4.com
.......
.......
.......
server200=nameofserver200.com
[file]
file1=/name/of/file/file1.properties
file2=/name/of/file/file2.xml
.......
.......
file5=/name/of/file/file5.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let us save the configuration file as servers_files.txt&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;PSC&#34;&gt;&lt;strong&gt;Python Script&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os							
import ConfigParser						
config = ConfigParser.ConfigParser()				
config.readfp(open(r&#39;servers_files.txt&#39;)) 				        
path1 = config.get(&#39;server&#39;,&#39;server1&#39;) 						  
for (file_key, file_val) in config.items(&amp;quot;file&amp;quot;): 			        
FILE_NAME= &amp;quot;$(echo {} | sed &#39;s!.*/!!&#39;)&amp;quot;.format(file_val)
	os.system(&amp;quot;mkdir -p /home/yuvraj/Desktop/{}&amp;quot;.format(FILE_NAME))	
	for (server_key, server_val) in config.items(&amp;quot;server&amp;quot;):
		os.system(&amp;quot;scp -oStrictHostKeyChecking=no username@{}:{} /home/yuvraj/Desktop/{}/{}&amp;quot;.format(server_val,file_val,FILE_NAME,server_val)) 	
		os.system(&amp;quot;diff -q /home/yuvraj/Desktop/{}/{} /home/yuvraj/Desktop/{}/{}&amp;quot;.format(FILE_NAME,path1,FILE_NAME,server_val))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a name=&#34;EPS&#34;&gt;&lt;strong&gt;Explanation of Python Script&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Line1 &amp;amp; Line 2 are used to import library for os operations and configuration parsing.&lt;/p&gt;

&lt;p&gt;Line 3 is used to call configparser object.&lt;/p&gt;

&lt;p&gt;Line 4 is used to open the txt file which is read by configparser.&lt;/p&gt;

&lt;p&gt;Line 5 is used to fetch the value of server1 key.&lt;/p&gt;

&lt;p&gt;Line 6 is used to create a loop to fetch file key &amp;amp; file value from configuration file.&lt;/p&gt;

&lt;p&gt;Line 7 is used to fetch only the filename from the filelocation present in configuration file.&lt;/p&gt;

&lt;p&gt;Line 8 is to create the directory based on filename fetched.&lt;/p&gt;

&lt;p&gt;Line 9 is used to create a loop to fetch server key &amp;amp; value.&lt;/p&gt;

&lt;p&gt;Line 10 is used to provide scp to copy file from server to local.&lt;/p&gt;

&lt;p&gt;Line 11 used to find differences between all the files copied to local.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;AAS&#34;&gt;&lt;strong&gt;Advantages of Automation Script&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No manual task&lt;/li&gt;
&lt;li&gt;Easy to catch the problem by catching proper exception such as file not present in desired location or file contents differ&lt;/li&gt;
&lt;li&gt;Verify within minutes that cluster of 200 node contains correct configuration files&lt;/li&gt;
&lt;li&gt;Easy to verify whether contents of configuration file are same or not&lt;/li&gt;
&lt;li&gt;Easy to fix the problem as cause of problem is known and minimum downtime will be required to fix the problem if caused&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This usecase is not limited to the following scenarios:-&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Addition of nodes in cluster&lt;/li&gt;
&lt;li&gt;Removal of nodes in cluster&lt;/li&gt;
&lt;li&gt;Setting up new cluster&lt;/li&gt;
&lt;li&gt;Upgrading the version of the software used in cluster across the nodes&lt;/li&gt;
&lt;li&gt;Downgrading the version of the software used in cluster across the nodes&lt;/li&gt;
&lt;li&gt;Installing new software requiring setting up configuration files across the nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the end of this blog you will understand the ideology behind using automation for Big Data Testing. You will discover the benefits of using automation to test in Big Data ecosystem which will reduce the manual intervention of work to an extent. At the end a sample Python Script was shown which solved the problem in a simple yet elegant manner.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data Testing</title>
      <link>http://guptayuvraj.github.io/post/Big-Data-Testing/</link>
      <pubDate>Thu, 04 Aug 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/Big-Data-Testing/</guid>
      <description>&lt;p&gt;This blog is for people who want to understand what to test in the Big Data ecosystem or what are the scenarios to cover in Big Data Testing. We will cover the following topics:-&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#BD&#34;&gt;Brief understanding on what is Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#BDT&#34;&gt;Why require Big Data Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#BDA&#34;&gt;High level architecture of Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#BDC&#34;&gt;Brief explanation of Big Data Components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#BDTS&#34;&gt;Big Data Testing scenarios&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#DI&#34;&gt;Data Ingestion Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#DP&#34;&gt;Data Processing Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#DS&#34;&gt;Data Storage Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#OTS&#34;&gt;Other Testing scenarios&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#TOOLS&#34;&gt;List of few tools used in Big Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&#34;BD&#34;&gt; &lt;strong&gt;What is Big Data?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Big Data is the new buzzword in the industry primarily due to large amount of data generated daily. Big Data is used to describe data which is large in size and grows exponentially with time. Big Data is based on 4V’s Volume (amount of data), Velocity (Speed of data in and out), Variety (Range of data type and sources) and Veracity (uncertainty of data). As data increases it becomes difficult to process, handle and manage the data. While traditional Computing infrastructure cannot work efficiently to handle Big Data, New Computing technologies have been created to handle, manage huge amount of data and processing it quicker than the traditional system and technologies.&lt;/p&gt;

&lt;p&gt;As Enterprises started to move towards Big Data it becomes important to understand the system and technologies used in order to get the best out of it. Enterprises have a new learning curve while moving towards Big Data. Learning the technologies is just a starting block whereas designing, testing and implementing are the big challenges to consider while moving to a whole new technology.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;BDT&#34;&gt; &lt;strong&gt;Why require Big Data Testing?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With introduction of Big Data it becomes very much important to test the big data system with usage of appropriate data correctly. If not tested properly it would affect the business significantly thus automation becomes a key part of Big Data Testing to test the application and it’s functionality. Big Data Testing if done incorrectly will make it very difficult to understand the error, how it occurred and the probable solution with mitigation steps could take a long time thus resulting in incorrect/missing data and correcting it is again a huge challenge in such a way that current flowing data is not affected. As data is very important it is recommended to have a proper mechanism so that data is not lost/corrupted and proper mechanism should be used to handle failovers.&lt;/p&gt;

&lt;p&gt;We will be primarily discussing about Big Data Testing in Hadoop. Big Data primarily uses Hadoop for processing and handling large amount of data. Hadoop is a framework which provides cluster of computing resources for processing huge amount of data. In Hadoop extending cluster is easy with addition of nodes as required which should be carefully planned during design /requirement stage.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;BDA&#34;&gt; &lt;strong&gt;Big Data Architecture&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let us have a look at the high level Big Data architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/BigDataArchitecture.png&#34; alt=&#34;Big Data Architecture&#34; title=&#34;Big Data Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the above diagram, Data Storage block contains Data Ingestion layer and Data Processing layer which are used to store processed data. Ingested data is stored in HDFS which acts as input for data processing. The above diagram also shows the data pipeline from data ingestion to data visualization.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;BDC&#34;&gt;&lt;strong&gt;Explanation of Big Data Components&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Data ingestion layer is responsible for ingesting data into Hadoop. It is a preprocessing stage and the entry point from which data comes. It is used for batch, file or event ingestion. This layer is very critical because if data is corrupted or missing then data cannot be processed leading to complete loss of data. Handling failures /failover is very important to manage. In this layer storage formats play a crucial role for compression of data which would lead to reduction in I/O.&lt;/p&gt;

&lt;p&gt;Data Processing layer is responsible for processing of data ingested, aggregation of data as per business requirements. This layer uses business rules for processing and aggregating the data. Hadoop is used for processing data which uses Map Reduce operations. It is very important to create proper alert mechanisms in order to catch the failure and helping in resolving it as soon as possible.&lt;/p&gt;

&lt;p&gt;Data Storage layer is responsible to store the processed data from Hadoop. As data generated is huge it becomes important to design and use this layer in order to store all the data. This needs to be designed very carefully to prevent disk corruption or other failures leading to loss of data. This layer is also referred to data warehouse for storing infrequently accessed data, archived data or old data.&lt;/p&gt;

&lt;p&gt;Data Visualization layer is responsible for visualizing the data received from ingestion, processed as per business rules and storing the data. It is used for understanding the data and gathering insights from data. Stored data can be in any format (Excel file, Json file, Text file, Access file etc.)  which is used for visualization of data. Also it is not necessary that only data stored in HDFS can be used for data visualization.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;BDTS&#34;&gt;&lt;strong&gt;Big Data Testing Scenarios&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let us examine the scenarios for which Big Data Testing can be used in the Big Data components:-&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;DI&#34;&gt;&lt;strong&gt;Data Ingestion&lt;/strong&gt; :-&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This step is considered as pre-Hadoop stage where data is generated from multiple sources and data flows into HDFS. In this step the testers verifies that data is extracted properly and data is loaded into HDFS.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ensure proper data from multiple data sources is ingested i.e. all required data is ingested as per their defined schema and data not matching schema should not be ingested. Data which has not matched with schema should be stored for stats reporting purpose. Also ensure there is no data corruption.&lt;/li&gt;
&lt;li&gt;Comparison of source data with data ingested to simply validate that correct data is pushed.&lt;/li&gt;
&lt;li&gt;Verify that correct data files are generated and loaded into HDFS correctly into desired location.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&#34;DP&#34;&gt;&lt;strong&gt;Data Processing&lt;/strong&gt; :-&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This step is used for validating Map-Reduce jobs. Map-Reduce is a concept used for condensing large amount of data into aggregated data. The data ingested is processed using execution of Map-Reduce jobs which provides desired results. In this step the tester verifies that ingested data is processed using Map-Reduce jobs and validate whether business logic is implemented correctly.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ensure Map Reduce Jobs run properly without any exceptions.&lt;/li&gt;
&lt;li&gt;Ensure key-value pairs are correctly generated post MR Jobs.&lt;/li&gt;
&lt;li&gt;Validate business rules are implemented on data.&lt;/li&gt;
&lt;li&gt;Validate data aggregation is implemented on data and data is consolidated post reduce operations.&lt;/li&gt;
&lt;li&gt;Validate that data is processed correctly post Map-Reduce Jobs by comparing output files with input files.
&amp;gt; Note: - For validation of data ingestion or data processing layers, we should use a small set of sample data (in Kb’s or Mb’s). By using a small sample data we can easily verify that correct data is ingested by comparing source data with data ingested output data at ingestion layer.  It becomes easier to verify that MR jobs are run without any error, business rules are correctly implemented on ingested data and validate data aggregation is correctly done by comparing output file with input file.
&amp;gt; &lt;strong&gt;Initially for testing data ingestion or data processing layers if using large data (in GB’s), it becomes very difficult to validate or verify each input record with output record and validating whether business rules are implemented correctly becomes difficult.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&#34;DS&#34;&gt;&lt;strong&gt;Data Storage&lt;/strong&gt; :-&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This step is used for storing output data in HDFS or any other storage system (such as Data Warehouse). In this step the tester verifies that output data is correctly generated and loaded into storage system.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Validate data is aggregated post Map-Reduce Jobs.&lt;/li&gt;
&lt;li&gt;Verify that correct data is loaded into storage system &amp;amp; discard any intermediate data which is present.&lt;/li&gt;
&lt;li&gt;Verify that there is no data corruption by comparing output data with HDFS (or any storage system) data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&#34;OTS&#34;&gt;The other type of testing scenarios a Big Data Tester can do is:-&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Check whether proper alert mechanisms are implemented such as Mail on alert, sending metrics on Cloud watch etc.&lt;/li&gt;
&lt;li&gt;Check Exceptions or errors are displayed properly with appropriate exception message so that solving an error becomes easy.&lt;/li&gt;
&lt;li&gt;Performance testing to test the different parameters to process a random chunk of large data and monitor parameters such as time taken to complete Map-Reduce Jobs, memory utilization, disk utilization and other metrics as required.&lt;/li&gt;
&lt;li&gt;Integration testing for testing complete workflow directly from data ingestion to data storage/visualization.&lt;/li&gt;
&lt;li&gt;Architecture testing for testing that Hadoop is highly available all the time &amp;amp; Failover services are properly implemented to ensure data is processed even in case of failure of nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: - For testing it is very important to generate data for testing covering various test scenarios (positive and negative). Positive test scenarios cover scenarios which are directly related to the functionality. Negative test scenarios cover scenarios which do not have direct relation with the desired functionality.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;TOOLS&#34;&gt;&lt;strong&gt;List of few tools used in Big Data&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Ingestion&lt;/strong&gt; - Kafka, Zookeeper, Sqoop, Flume, Storm, Amazon Kinesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Processing&lt;/strong&gt; - Hadoop (Map-Reduce), Cascading, Oozie, Hive, Pig.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Storage&lt;/strong&gt; - HDFS (Hadoop Distributed File System), Amazon S3, HBase.&lt;/p&gt;

&lt;p&gt;At the end of this blog you understand the various scenarios which can be tested in Big Data domain.  You know what is Big Data and why do we require Big Data Testing. It made you aware of the Big Data architecture with brief explanation of its components. Lastly few tools were mentioned which are used within the Big Data System.&lt;/p&gt;

&lt;p&gt;In the next blog we will look at a use-case for a practical scenario as an example for Big Data Testing. It will cover the problem statement followed by testing via traditional method and importance of creating/using automation script to automate testing in Big Data domain.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
