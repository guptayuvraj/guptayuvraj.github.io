<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Software on Blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://guptayuvraj.github.io/categories/software/</link>
    <language>en-us</language>
    <author>Yuvraj Gupta</author>
    
    <updated>Sun, 17 Apr 2016 00:00:00 UTC</updated>
    
    <item>
      <title>JedaNetwork</title>
      <link>http://guptayuvraj.github.io/post/JedaNetwork/</link>
      <pubDate>Sun, 17 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/JedaNetwork/</guid>
      <description>&lt;p&gt;Jeda Networks game changing intelligent software product called the Fabric Network Controller (FNC) abstracts the complexity of a storage network into a software controller.&lt;/p&gt;

&lt;p&gt;The Jeda FNC creates a high performance “storage network overlay” on top of an Ethernet fabric. This Software Defined Storage Network (SDSN) transforms the Ethernet fabric into a powerful and agile storage networking fabric. SDSNs solve the limitations of a fixed and rigid physical storage network—namely scalability, high cost, high complexity, and lack of agility&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Maximize Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cut Management Costs-&lt;/strong&gt; mapping your SAN requirements to the physical Ethernet network&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No Hardware Lock-In&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The FNC has been successfully integrated with a number of multi-vendor 10Gb, 40Gb and 100 Gb Data Center Ethernet switches and as FNC is independent of the network hardware the FNC is software, it is network and access speed agnostic, so as speeds migrate to 100Gbs and beyond, the FNC will continue to work within these leaps of performance., it is 10, 40 and 100Gb Ethernet ready today.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Stack Swift</title>
      <link>http://guptayuvraj.github.io/post/OpenStackSwift/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/OpenStackSwift/</guid>
      <description>&lt;p&gt;&lt;strong&gt;ELIMINATE VENDOR LOCK-IN&lt;/strong&gt; OpenStack Swift is open source.
Change providers without changing your application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LOW COST&lt;/strong&gt; Choice of industry standard hardware that is available from multiple vendors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Deployment Automation- Deployment of availability zones based on hardware.
Centralized Management- Provides diagnostics, monitoring data, alerts to controller.
Capacity Management- Orchestrates capacity additions and removals without interruption.
Monitoring- Monitors 500+ data points per node and integration with ganglia and nagios.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hardware Overview:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Proxy Tier- Proxy nodes handles all incoming API requests. Once a proxy server receives a request, it will determine which storage node to connect to, based on the URL of the object. A minimum of two nodes should be deployed in the proxy tier for redundancy. The proxy tier also includes ssl-terminators and authentication services.&lt;/p&gt;

&lt;p&gt;Storage Tier- When a client uploads data to SwiftStack, the proxy tier will write data in three different availability zones in the storage tier. For incoming read requests, one of the three replicas of the data will be served. Subsequently, each storage node and drive that is added to a Swift cluster will not only provide additional storage capacity, but will also increase the aggregate IO capacity of the cluster as there are more drives available to serve incoming read requests.&lt;/p&gt;

&lt;p&gt;Networking- A typical SwiftStack deployment will have a front-facing ‘access’ network (to run the proxy and authentication services) and a back-end ‘storage’ network.  As there are three copies of each object, an incoming write is sent to three storage nodes. Therefore network capacity for writes needs to be considered in proportion to overall workload.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recommended System Requirement:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Proxy Tier- Typically, Proxy servers are 1U systems with a minimum of 12 GB RAM. For small SwiftStack clusters, the storage services and proxy services can run on the same physical nodes.&lt;/p&gt;

&lt;p&gt;Storage Tier- Storage nodes are typically higher ­density 2U, 3U or 4U nodes with 12-­‐36 SATA disks each. A good rule of thumb is approximately 1 GB of RAM for each TB of Disk.&lt;/p&gt;

&lt;p&gt;Drives- 2TB or 3TB 7200 RPM SATA drives.&lt;/p&gt;

&lt;p&gt;Networking- Storage nodes can be provisioned with 1GbE (single gigabit) or 10GbE network interface depending on expected workload and desired performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NexentaStor</title>
      <link>http://guptayuvraj.github.io/post/NexentaStor/</link>
      <pubDate>Wed, 13 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/NexentaStor/</guid>
      <description>&lt;p&gt;Nexenta – NexentaStor:  delivers high-performance, ultra-scalable, cloud- and virtualization-optimized storage solutions. Nexenta supplies the software. You leverage your hardware. Does not provide object storage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Delivers built-proof data integrity and unlimited scalability for Big Data- and Cloud-optimized storage solutions. Nexenta’s OpenStorage platform allows for a broader range of virtualization and cloud-computing services at a much lower cost.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High-Availability Clustering.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unlimited Snapshots &amp;amp; Clones:&lt;/strong&gt; NexentaStor uses copy on write (COW), this means live data is never overwritten; instead, it writes data to a new block before changing the data pointers and committing the write.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inherent Virtualization&lt;/strong&gt; – NexentaStor supports all the mainstream virtualization vendors; Microsoft Hyper-V, Citrix Xen and VMware vSphere.
Uses ZFS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recommended system Requirement for NexentaStor Community Edition:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;64 bit processor (32 bit not recommended for production).&lt;/p&gt;

&lt;p&gt;2 GB RAM minimum for evaluation (recommend 4 to 8 GB), Production use 8 GB minimum plus 1 GB per 1 TB raw disk space minimum (depending on use-case more may be highly desirable), or 2 GB per 1 TB raw for high-end performance deployments. If planning to use deduplication, please contact for sizing assistance.&lt;/p&gt;

&lt;p&gt;2 identical relatively small disks for high-availability system folder.&lt;/p&gt;

&lt;p&gt;Additional drives/storage for data volumes&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding about EMC Vipr- All in One</title>
      <link>http://guptayuvraj.github.io/post/EMC-Vipr/</link>
      <pubDate>Tue, 12 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/EMC-Vipr/</guid>
      <description>&lt;p&gt;EMC ViPR will purely provide the Control Plane. It discovers storage, creates virtual storage pools, and provisions those pools to the application. Only by separating the data center from its underlying hardware can IT truly deliver resources as customizable, on-demand services. ViPR is able to support IT services in a heterogeneous storage environment while retaining and extending the value of underlying arrays. It perform functions like “in-place” analytics. Abstracting, pooling and automating the infrastructure. It supports open API including Amazon Simple storage service (S3), Open stack Swift &amp;amp; Atmos to completely free data and applications from storage dependencies.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Managing both storage infrastructure (called the Control Plane) and the data stored within that infrastructure (called the Data Plane).&lt;/li&gt;
&lt;li&gt;Decoupling the Control Plane from the Data Plane, allowing the use of both together — or enabling customers to use only the Control Plane to manage the underlying intelligence of the storage arrays through policy-based automation. This is a radical departure from prior attempts to virtualize storage.&lt;/li&gt;
&lt;li&gt;Offering the ability to view objects as files and provides file access performance without the latency inherent to object storage.&lt;/li&gt;
&lt;li&gt;Providing the capability to be implemented entirely in software and will run against EMC, non-EMC and commodity hardware.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;../images/EMC-Vipr.jpg&#34; alt=&#34;&#34; title=&#34;EMC Vipr&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is Software Defined Storage</title>
      <link>http://guptayuvraj.github.io/post/What-is-Software-Defined-Storage/</link>
      <pubDate>Mon, 11 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/What-is-Software-Defined-Storage/</guid>
      <description>&lt;p&gt;The concept behind software-defined storage seems very appealing: a single point of data services, like volume management and snapshots, form a single console while using &lt;strong&gt;practically anyone’s disk hardware.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a pillar in the SDDC stack, Software Defined Storage (SDS) pools hardware storage resources and allows them to be programmatically defined in software. All the programming that controls storage-related tasks exists solely in software. The software could exist on a server or could be part of an operating system (OS) or a hypervisor. Provides flexible management at a much more granular level. Deployed Provision through S/W.&lt;/p&gt;

&lt;p&gt;SDS forms a part of a broader software-defined data center (SDDC) concept wherein all the virtualized storage, server, networking and security resources required by an application can be defined by software and provisioned automatically. This design provides the means for storage services to be deployed on a wide range of hardware spanning vendor optimized to commodity to cloud.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
