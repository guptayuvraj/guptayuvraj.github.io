<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Storage on Blog </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://guptayuvraj.github.io/categories/storage/</link>
    <language>en-us</language>
    <author>Yuvraj Gupta</author>
    
    <updated>Sun, 15 May 2016 00:00:00 UTC</updated>
    
    <item>
      <title>ScaleIOFeatures</title>
      <link>http://guptayuvraj.github.io/post/ScaleIOFeatures/</link>
      <pubDate>Sun, 15 May 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/ScaleIOFeatures/</guid>
      <description>&lt;p&gt;ScaleIO ECS is a software-only solution that uses application hosts’ local disks.&lt;/p&gt;

&lt;p&gt;Features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simplified operation&lt;/strong&gt;- ECS enables IT administrator to manage the entire data center stack.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Avoid capacity planning&lt;/strong&gt;- ECS is elastic, enabling you to add, move, remove servers and capacity during I/O operations.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Self-healing&lt;/strong&gt;- It rebalances whenever capacity and/or servers are added.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dynamic performance allocation&lt;/strong&gt;- It provides dynamic allocation based on applications’ needs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Freedom of choice through hardware vendor lock-in elimination.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>JedaNetwork</title>
      <link>http://guptayuvraj.github.io/post/JedaNetwork/</link>
      <pubDate>Sun, 17 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/JedaNetwork/</guid>
      <description>&lt;p&gt;Jeda Networks game changing intelligent software product called the Fabric Network Controller (FNC) abstracts the complexity of a storage network into a software controller.&lt;/p&gt;

&lt;p&gt;The Jeda FNC creates a high performance “storage network overlay” on top of an Ethernet fabric. This Software Defined Storage Network (SDSN) transforms the Ethernet fabric into a powerful and agile storage networking fabric. SDSNs solve the limitations of a fixed and rigid physical storage network—namely scalability, high cost, high complexity, and lack of agility&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Maximize Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cut Management Costs-&lt;/strong&gt; mapping your SAN requirements to the physical Ethernet network&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No Hardware Lock-In&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The FNC has been successfully integrated with a number of multi-vendor 10Gb, 40Gb and 100 Gb Data Center Ethernet switches and as FNC is independent of the network hardware the FNC is software, it is network and access speed agnostic, so as speeds migrate to 100Gbs and beyond, the FNC will continue to work within these leaps of performance., it is 10, 40 and 100Gb Ethernet ready today.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Stack Swift</title>
      <link>http://guptayuvraj.github.io/post/OpenStackSwift/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/OpenStackSwift/</guid>
      <description>&lt;p&gt;&lt;strong&gt;ELIMINATE VENDOR LOCK-IN&lt;/strong&gt; OpenStack Swift is open source.
Change providers without changing your application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LOW COST&lt;/strong&gt; Choice of industry standard hardware that is available from multiple vendors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Deployment Automation- Deployment of availability zones based on hardware.
Centralized Management- Provides diagnostics, monitoring data, alerts to controller.
Capacity Management- Orchestrates capacity additions and removals without interruption.
Monitoring- Monitors 500+ data points per node and integration with ganglia and nagios.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hardware Overview:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Proxy Tier- Proxy nodes handles all incoming API requests. Once a proxy server receives a request, it will determine which storage node to connect to, based on the URL of the object. A minimum of two nodes should be deployed in the proxy tier for redundancy. The proxy tier also includes ssl-terminators and authentication services.&lt;/p&gt;

&lt;p&gt;Storage Tier- When a client uploads data to SwiftStack, the proxy tier will write data in three different availability zones in the storage tier. For incoming read requests, one of the three replicas of the data will be served. Subsequently, each storage node and drive that is added to a Swift cluster will not only provide additional storage capacity, but will also increase the aggregate IO capacity of the cluster as there are more drives available to serve incoming read requests.&lt;/p&gt;

&lt;p&gt;Networking- A typical SwiftStack deployment will have a front-facing ‘access’ network (to run the proxy and authentication services) and a back-end ‘storage’ network.  As there are three copies of each object, an incoming write is sent to three storage nodes. Therefore network capacity for writes needs to be considered in proportion to overall workload.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recommended System Requirement:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Proxy Tier- Typically, Proxy servers are 1U systems with a minimum of 12 GB RAM. For small SwiftStack clusters, the storage services and proxy services can run on the same physical nodes.&lt;/p&gt;

&lt;p&gt;Storage Tier- Storage nodes are typically higher ­density 2U, 3U or 4U nodes with 12-­‐36 SATA disks each. A good rule of thumb is approximately 1 GB of RAM for each TB of Disk.&lt;/p&gt;

&lt;p&gt;Drives- 2TB or 3TB 7200 RPM SATA drives.&lt;/p&gt;

&lt;p&gt;Networking- Storage nodes can be provisioned with 1GbE (single gigabit) or 10GbE network interface depending on expected workload and desired performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NexentaStor</title>
      <link>http://guptayuvraj.github.io/post/NexentaStor/</link>
      <pubDate>Wed, 13 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/NexentaStor/</guid>
      <description>&lt;p&gt;Nexenta – NexentaStor:  delivers high-performance, ultra-scalable, cloud- and virtualization-optimized storage solutions. Nexenta supplies the software. You leverage your hardware. Does not provide object storage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Delivers built-proof data integrity and unlimited scalability for Big Data- and Cloud-optimized storage solutions. Nexenta’s OpenStorage platform allows for a broader range of virtualization and cloud-computing services at a much lower cost.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High-Availability Clustering.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unlimited Snapshots &amp;amp; Clones:&lt;/strong&gt; NexentaStor uses copy on write (COW), this means live data is never overwritten; instead, it writes data to a new block before changing the data pointers and committing the write.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inherent Virtualization&lt;/strong&gt; – NexentaStor supports all the mainstream virtualization vendors; Microsoft Hyper-V, Citrix Xen and VMware vSphere.
Uses ZFS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recommended system Requirement for NexentaStor Community Edition:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;64 bit processor (32 bit not recommended for production).&lt;/p&gt;

&lt;p&gt;2 GB RAM minimum for evaluation (recommend 4 to 8 GB), Production use 8 GB minimum plus 1 GB per 1 TB raw disk space minimum (depending on use-case more may be highly desirable), or 2 GB per 1 TB raw for high-end performance deployments. If planning to use deduplication, please contact for sizing assistance.&lt;/p&gt;

&lt;p&gt;2 identical relatively small disks for high-availability system folder.&lt;/p&gt;

&lt;p&gt;Additional drives/storage for data volumes&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding about EMC Vipr- All in One</title>
      <link>http://guptayuvraj.github.io/post/EMC-Vipr/</link>
      <pubDate>Tue, 12 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/EMC-Vipr/</guid>
      <description>&lt;p&gt;EMC ViPR will purely provide the Control Plane. It discovers storage, creates virtual storage pools, and provisions those pools to the application. Only by separating the data center from its underlying hardware can IT truly deliver resources as customizable, on-demand services. ViPR is able to support IT services in a heterogeneous storage environment while retaining and extending the value of underlying arrays. It perform functions like “in-place” analytics. Abstracting, pooling and automating the infrastructure. It supports open API including Amazon Simple storage service (S3), Open stack Swift &amp;amp; Atmos to completely free data and applications from storage dependencies.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Managing both storage infrastructure (called the Control Plane) and the data stored within that infrastructure (called the Data Plane).&lt;/li&gt;
&lt;li&gt;Decoupling the Control Plane from the Data Plane, allowing the use of both together — or enabling customers to use only the Control Plane to manage the underlying intelligence of the storage arrays through policy-based automation. This is a radical departure from prior attempts to virtualize storage.&lt;/li&gt;
&lt;li&gt;Offering the ability to view objects as files and provides file access performance without the latency inherent to object storage.&lt;/li&gt;
&lt;li&gt;Providing the capability to be implemented entirely in software and will run against EMC, non-EMC and commodity hardware.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;../images/EMC-Vipr.jpg&#34; alt=&#34;&#34; title=&#34;EMC Vipr&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is Software Defined Storage</title>
      <link>http://guptayuvraj.github.io/post/What-is-Software-Defined-Storage/</link>
      <pubDate>Mon, 11 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/What-is-Software-Defined-Storage/</guid>
      <description>&lt;p&gt;The concept behind software-defined storage seems very appealing: a single point of data services, like volume management and snapshots, form a single console while using &lt;strong&gt;practically anyone’s disk hardware.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a pillar in the SDDC stack, Software Defined Storage (SDS) pools hardware storage resources and allows them to be programmatically defined in software. All the programming that controls storage-related tasks exists solely in software. The software could exist on a server or could be part of an operating system (OS) or a hypervisor. Provides flexible management at a much more granular level. Deployed Provision through S/W.&lt;/p&gt;

&lt;p&gt;SDS forms a part of a broader software-defined data center (SDDC) concept wherein all the virtualized storage, server, networking and security resources required by an application can be defined by software and provisioned automatically. This design provides the means for storage services to be deployed on a wide range of hardware spanning vendor optimized to commodity to cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best Practices while going for Storage Virtualization</title>
      <link>http://guptayuvraj.github.io/post/Best-Practices-Storage-Virtualization/</link>
      <pubDate>Sat, 09 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/Best-Practices-Storage-Virtualization/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Set your scope and objectives.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Begin the vendor selection process with specific goals and objectives. We must fully understand what we want to achieve or overcome with storage virtualization.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Arrange Proper Staff Training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Address skill gaps so we can properly asses different virtualization approaches. Storage virtualization requires specific training and knowledge, so it’s necessary to asses current IT staff for skills and identify potential knowledge gaps. Also consider vendor specific training once finalized.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Leverage Storage Resource Management (SRM)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Properly instrumenting SRM tools providing full discovery and visibility of virtual, logic and physical storage.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Establish Storage Management Standards&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reduce costs associated with managing multiple site management. When you establish storage management standards including standard LUN sizes and naming conventions—you can more easily interchange similar storage subsystems, reducing complicated planning exercises. Standard configurations simplify both production environment planning and disaster recovery site recovery planning.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Implement Service Maintenance Best Practices&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Accurately calculate capital and operational expenses for applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implement Storage Virtualization</title>
      <link>http://guptayuvraj.github.io/post/Implement-Storage-Virtualization/</link>
      <pubDate>Fri, 08 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/Implement-Storage-Virtualization/</guid>
      <description>&lt;p&gt;Why do we need to implement Storage Virtualization?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Increasing Storage Demands&lt;/li&gt;
&lt;li&gt;Too long to do backup and restore.&lt;/li&gt;
&lt;li&gt;Limited Space in Data Center.&lt;/li&gt;
&lt;li&gt;Need to improve DR capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Approach for implementation of Storage Virtualization?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Host Based:&lt;/strong&gt;  Physical drives are handled by traditional device driver, while a software layer above device driver intercepts I/O requests, looks up metadata and redirects I/O.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simple to design and code.&lt;/li&gt;
&lt;li&gt;Supports any storage type.&lt;/li&gt;
&lt;li&gt;Improves storage utilization without thin provisioning restrictions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Replication and data migration only possible locally to that host.&lt;/li&gt;
&lt;li&gt;Traditional data recovery following a server disk crash is impossible.&lt;/li&gt;
&lt;li&gt;Storage utilization optimized only on per host basis.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Network Based:&lt;/strong&gt;  Storage virtualization is viewed as a network-based device using fibre channel networks connected as SAN. A fibre channel switch which is in between the host and storage will virtualize all requests. Interestingly operating system on the host doesn’t know it’s happening as it this method doesn’t rely on operating system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simple Management Interface.&lt;/li&gt;
&lt;li&gt;Replication services across heterogeneous devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Difficult to implement fast metadata updates in switches.&lt;/li&gt;
&lt;li&gt;Out-of-band requires specific host based software.&lt;/li&gt;
&lt;li&gt;In-band adds latency to I/O.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Implementation of Network based&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Appliance Based:&lt;/strong&gt; an appliance residing between client and storage array implements storage virtualization layer and becomes new storage target. When the data is sent to appliance via client storage virtualization features can be enabled such as snapshot, thin provisioning and data replication.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Versatility&lt;/li&gt;
&lt;li&gt;Cost&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Switch Based:&lt;/strong&gt; an application that resides in server connected to fibre channel SAN switch. Though it sits in between host and storage but it may use different techniques for metadata mapping.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Faster Speed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Expensive.&lt;/li&gt;
&lt;li&gt;Lacks features such as thin provisioning.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Storage Device Based:&lt;/strong&gt; the primary storage controller allows direct attachment of other storage controllers and provides virtualization services. It will provide pooling and metadata management services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No additional hardware or infrastructure requirement.&lt;/li&gt;
&lt;li&gt;Provides most of the benefits of storage virtualization.&lt;/li&gt;
&lt;li&gt;Does not add latency to individual I/O.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Storage utilization optimized only across the connected controllers&lt;/li&gt;
&lt;li&gt;Replication and data migration only possible across the connected controllers and same vendors device for long distance support&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Array Based:&lt;/strong&gt; storage virtualization layer lives entirely within storage array.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Challenges  of Storage Virtualization</title>
      <link>http://guptayuvraj.github.io/post/Challenges-Storage-Virtualization/</link>
      <pubDate>Thu, 07 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/Challenges-Storage-Virtualization/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;Storage Architecture becomes  much more complicated.&lt;/li&gt;
&lt;li&gt;More complication may lead to troubleshooting problems.&lt;/li&gt;
&lt;li&gt;Problem determination and fault isolation can become complex.&lt;/li&gt;
&lt;li&gt;Arise of situations in which physical storage resources are overcommitted.&lt;/li&gt;
&lt;li&gt;Backing out of failed implementation.&lt;/li&gt;
&lt;li&gt;Interoperability.&lt;/li&gt;
&lt;li&gt;Management of Meta Data.&lt;/li&gt;
&lt;li&gt;Higher Latency&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Benefits of Storage Virtualization</title>
      <link>http://guptayuvraj.github.io/post/Benefits-Storage-Virtualization/</link>
      <pubDate>Wed, 06 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/Benefits-Storage-Virtualization/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;Improved Storage Management in an IT environment&lt;/li&gt;
&lt;li&gt;Better availability and estimation of downtime required with automated management.&lt;/li&gt;
&lt;li&gt;Ability to migrate data while retaining  I/O access (Non disruptive data migration).&lt;/li&gt;
&lt;li&gt;Reduce the number of SAN devices.&lt;/li&gt;
&lt;li&gt;Decrease in Power consumed.&lt;/li&gt;
&lt;li&gt;Time Saving.&lt;/li&gt;
&lt;li&gt;Ease in Data Replication.&lt;/li&gt;
&lt;li&gt;Easy backup, archiving and recovery tasks.&lt;/li&gt;
&lt;li&gt;Expansion of Storage Capacity.&lt;/li&gt;
&lt;li&gt;Easy Updates&lt;/li&gt;
&lt;li&gt;Ability to increase Storage Volume Size in real time.&lt;/li&gt;
&lt;li&gt;Increased loading and backup speed.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>What is Storage Virtualization</title>
      <link>http://guptayuvraj.github.io/post/What-is-Storage-Virtualization/</link>
      <pubDate>Tue, 05 Apr 2016 00:00:00 UTC</pubDate>
      <author>Yuvraj Gupta</author>
      <guid>http://guptayuvraj.github.io/post/What-is-Storage-Virtualization/</guid>
      <description>&lt;p&gt;It is the abstraction of physical media (hard disks or solid state drives etc) from the attaching servers which treats the storage as a single logical entity without regard to the hierarchy of physical media that is involved. Storage virtualization means adding an abstraction layer of software that hides the physical devices from the user and allows all devices to be managed as a single pool.
It adds a new layer of software or hardware in between the storage systems and servers so that the applications would not require to know to which logical drive, partitions or storage subsystems the data resides in.&lt;/p&gt;

&lt;p&gt;Within the context of a storage system, there are two primary types of virtualization that can occur:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Block virtualization&lt;/strong&gt; used in this context refers to the abstraction (separation) of logical storage (partition) from physical storage so that it may be accessed without regard to physical storage or heterogeneous structure. This separation allows the administrators of the storage system greater flexibility in how they manage storage for end users. The act of applying virtualization , to one or more block based (storage) services for the purpose of providing a new aggregated, higher level, richer, simpler, secure, etc., block service to clients. Block virtualization functions can be nested. A disk drive, RAID system or volume manager all perform some form of block address to (different) block address mapping or aggregation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File virtualization&lt;/strong&gt; addresses the NAS challenges by eliminating the dependencies between the data accessed at the file level and the location where the files are physically stored. This provides opportunities to optimize storage use and server consolidation and to perform non-disruptive file migrations. File virtualization is the creation of an abstraction layer between file servers and the clients that access those file servers. Once deployed, the file virtualization layer manages files and file systems across servers, allowing administrators to present clients with one logical file mount for all servers&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
